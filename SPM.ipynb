{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Reference: https://github.com/CyrusChiu/Image-recognition\n",
    "\n",
    "import cv2\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "import scipy.cluster.vq as vq\n",
    "\n",
    "import numpy as np\n",
    "import struct\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn import neighbors, metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.fromstring(f.read(), dtype=np.uint8).reshape(shape)\n",
    "\n",
    "def extract_DenseSift_descriptors(img):\n",
    "\n",
    "    gray = img \n",
    "\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    disft_step_size = DSIFT_STEP_SIZE\n",
    "    keypoints = [cv2.KeyPoint(x, y, disft_step_size)\n",
    "            for y in range(0, gray.shape[0], disft_step_size)\n",
    "                for x in range(0, gray.shape[1], disft_step_size)]\n",
    "\n",
    "    keypoints, descriptors = sift.compute(gray, keypoints)\n",
    "\n",
    "    return [keypoints, descriptors]\n",
    "\n",
    "def svm_classifier(x_train, y_train, x_test=None, y_test=None):\n",
    "\n",
    "    C_range = 10.0 ** np.arange(-3, 3)\n",
    "    gamma_range = 10.0 ** np.arange(-3, 3)\n",
    "    param_grid = dict(gamma=gamma_range.tolist(), C=C_range.tolist())\n",
    "\n",
    "    # Grid search for C, gamma, 5-fold CV\n",
    "    print(\"Tuning hyper-parameters\\n\")\n",
    "    clf = GridSearchCV(svm.SVC(), param_grid, cv=5, n_jobs=-2)\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"Best parameters set found on development set:\\n\")\n",
    "    print(clf.best_estimator_)\n",
    "    print(\"\\nGrid scores on development set:\\n\")\n",
    "    for params, mean_score, scores in clf.grid_scores_:\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean_score, scores.std() * 2, params))\n",
    "    print(\"\\nDetailed classification report:\\n\")\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\\n\")\n",
    "    y_true, y_pred = y_test, clf.predict(x_test)\n",
    "    #print(classification_report(y_true, y_pred, target_names=get_label()))\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    return y_true, y_pred\n",
    "\n",
    "def build_codebook(X, voc_size):\n",
    "\n",
    "    features = np.vstack((descriptor for descriptor in X))\n",
    "    kmeans = KMeans(n_clusters=voc_size, n_jobs=-2)\n",
    "    kmeans.fit(features)\n",
    "    codebook = kmeans.cluster_centers_.squeeze()\n",
    "    print(\"Codebook Building Complete\")\n",
    "    return codebook\n",
    "\n",
    "def input_vector_encoder(feature, codebook):\n",
    "    \"\"\"\n",
    "    Input all the local feature of the image\n",
    "    Pooling (encoding) by codebook and return\n",
    "    \"\"\"\n",
    "    code, _ = vq.vq(feature, codebook)\n",
    "    word_hist, bin_edges = np.histogram(code, bins=range(codebook.shape[0] + 1), normed=True)\n",
    "    return word_hist\n",
    "\n",
    "def build_spatial_pyramid(image, descriptor, level):\n",
    "    \"\"\"\n",
    "    Rebuild the descriptors according to the level of pyramid\n",
    "    \"\"\"\n",
    "    assert 0 <= level <= 2, \"Level Error\"\n",
    "    step_size = DSIFT_STEP_SIZE\n",
    "    #from utils import DSIFT_STEP_SIZE as s\n",
    "    s = 4\n",
    "    assert s == step_size, \"step_size must equal to DSIFT_STEP_SIZE                            in utils.extract_DenseSift_descriptors()\"\n",
    "    h = image.shape[0] // step_size\n",
    "    w = image.shape[1] // step_size\n",
    "    idx_crop = np.array(range(len(descriptor))).reshape(h,w)\n",
    "    size = idx_crop.itemsize\n",
    "    height, width = idx_crop.shape\n",
    "    bh, bw = 2**(3-level), 2**(3-level)\n",
    "    shape = (height//bh, width//bw, bh, bw)\n",
    "    strides = size * np.array([width*bh, bw, width, 1])\n",
    "    crops = np.lib.stride_tricks.as_strided(\n",
    "            idx_crop, shape=shape, strides=strides)\n",
    "    des_idxs = [col_block.flatten().tolist() for row_block in crops\n",
    "                for col_block in row_block]\n",
    "    pyramid = []\n",
    "    for idxs in des_idxs:\n",
    "        pyramid.append(np.asarray([descriptor[idx] for idx in idxs]))\n",
    "    return pyramid\n",
    "\n",
    "\n",
    "\n",
    "def spatial_pyramid_matching(image, descriptor, codebook, level):\n",
    "    pyramid = []\n",
    "    if level == 0:\n",
    "        pyramid += build_spatial_pyramid(image, descriptor, level=0)\n",
    "        code = [input_vector_encoder(crop, codebook) for crop in pyramid]\n",
    "        return np.asarray(code).flatten()\n",
    "    if level == 1:\n",
    "        pyramid += build_spatial_pyramid(image, descriptor, level=0)\n",
    "        pyramid += build_spatial_pyramid(image, descriptor, level=1)\n",
    "        code = [input_vector_encoder(crop, codebook) for crop in pyramid]\n",
    "        code_level_0 = 0.5 * np.asarray(code[0]).flatten()\n",
    "        code_level_1 = 0.5 * np.asarray(code[1:]).flatten()\n",
    "        return np.concatenate((code_level_0, code_level_1))\n",
    "    if level == 2:\n",
    "        pyramid += build_spatial_pyramid(image, descriptor, level=0)\n",
    "        pyramid += build_spatial_pyramid(image, descriptor, level=1)\n",
    "        pyramid += build_spatial_pyramid(image, descriptor, level=2)\n",
    "        code = [input_vector_encoder(crop, codebook) for crop in pyramid]\n",
    "        code_level_0 = 0.25 * np.asarray(code[0]).flatten()\n",
    "        code_level_1 = 0.25 * np.asarray(code[1:5]).flatten()\n",
    "        code_level_2 = 0.5 * np.asarray(code[5:]).flatten()\n",
    "        return np.concatenate((code_level_0, code_level_1, code_level_2))\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    \n",
    "    plt.figure(figsize = (8,8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "#     plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "(10000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n",
      "(10000,)\n",
      "Train/Test split: 10000/10000\n",
      "Codebook Size: 100\n",
      "Pyramid level: 1\n",
      "Building the codebook, it will take some time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/pairwise.py:257: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return distances if squared else np.sqrt(distances, out=distances)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/pairwise.py:257: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return distances if squared else np.sqrt(distances, out=distances)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/pairwise.py:257: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return distances if squared else np.sqrt(distances, out=distances)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/pairwise.py:257: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return distances if squared else np.sqrt(distances, out=distances)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/pairwise.py:257: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return distances if squared else np.sqrt(distances, out=distances)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/pairwise.py:257: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return distances if squared else np.sqrt(distances, out=distances)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/pairwise.py:257: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return distances if squared else np.sqrt(distances, out=distances)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/pairwise.py:257: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return distances if squared else np.sqrt(distances, out=distances)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/pairwise.py:257: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return distances if squared else np.sqrt(distances, out=distances)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/pairwise.py:257: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return distances if squared else np.sqrt(distances, out=distances)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codebook Building Complete\n"
     ]
    }
   ],
   "source": [
    "from utils import load_mnist\n",
    "\n",
    "(train_raw, train_label), (test_raw, test_label) = load_mnist()\n",
    "train_data = np.reshape(train_raw,(60000,28*28))\n",
    "test_data = np.reshape(test_raw,(10000,28*28))\n",
    "\n",
    "x_train = [np.uint8(np.reshape(img,(28,28))*255) for img in train_data]\n",
    "y_train = np.array([int(np.where(train_label[i,] == 1)[0]) for i in range(len(train_label))]).T\n",
    "x_test = [np.uint8(np.reshape(img,(28,28))*255) for img in test_data]\n",
    "y_test = np.array([int(np.where(test_label[i,] == 1)[0]) for i in range(len(test_label))]).T\n",
    "\n",
    "\n",
    "\n",
    "x_train = np.asarray(x_train)\n",
    "x_test = np.asarray(x_test)\n",
    "\n",
    "\n",
    "x_train = x_train[0:10000,:]\n",
    "x_test = x_test\n",
    "y_train = y_train[0:10000]\n",
    "y_test = y_test\n",
    "print (x_train.shape)\n",
    "print (x_test.shape)\n",
    "print (y_train.shape)\n",
    "print (y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "VOC_SIZE = 100\n",
    "PYRAMID_LEVEL = 1\n",
    "DSIFT_STEP_SIZE = 4\n",
    "\n",
    "x_train_feature = [extract_DenseSift_descriptors(img) for img in x_train]\n",
    "x_test_feature = [extract_DenseSift_descriptors(img) for img in x_test]\n",
    "x_train_kp, x_train_des = zip(*x_train_feature)\n",
    "x_test_kp, x_test_des = zip(*x_test_feature)\n",
    "print (\"Train/Test split: {:d}/{:d}\".format(len(y_train), len(y_test)))\n",
    "print (\"Codebook Size: {:d}\".format(VOC_SIZE))\n",
    "print (\"Pyramid level: {:d}\".format(PYRAMID_LEVEL))\n",
    "print (\"Building the codebook, it will take some time\")\n",
    "codebook = build_codebook(x_train_des, VOC_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial Pyramid Matching encoding\n",
      "(10000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "with open('./spm_lv1_codebook.pkl','wb') as f:\n",
    "    pickle.dump(codebook, f)\n",
    "\n",
    "print (\"Spatial Pyramid Matching encoding\")\n",
    "print (x_train.shape)\n",
    "print (x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [spatial_pyramid_matching(x_train[i],\n",
    "                                    x_train_des[i],\n",
    "                                    codebook,\n",
    "                                    level=PYRAMID_LEVEL)\n",
    "                                    for i in range(len(x_train))]\n",
    "\n",
    "x_test = [spatial_pyramid_matching(x_test[i],x_test_des[i],codebook,level=PYRAMID_LEVEL) for i in range(len(x_test))]\n",
    "\n",
    "x_train = np.asarray(x_train)\n",
    "x_test = np.asarray(x_test)\n",
    "\n",
    "y_true,y_pred = svm_classifier(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "# plot_confusion_matrix(cm,[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"],'./SPM/Confusion_matrix.png')\n",
    "plot_confusion_matrix(cm,[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
